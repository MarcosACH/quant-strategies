{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2164450f",
   "metadata": {},
   "source": [
    "# Data Configuration & Preparation Interface\n",
    "\n",
    "This notebook provides an interactive interface for configuring and preparing market data for quantitative strategy development.\n",
    "\n",
    "## Workflow Integration\n",
    "\n",
    "This notebook corresponds to **Phase 1** of the development workflow:\n",
    "- Data Collection & Preparation\n",
    "- Data Splitting Strategy\n",
    "- Data Quality Validation\n",
    "\n",
    "The prepared datasets will be used in subsequent phases for strategy development, optimization, and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64021c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import polars as pl\n",
    "import json\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(os.getcwd()).parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import our modules\n",
    "from src.data.config.data_config import DataConfig, DataSplitConfig\n",
    "from src.data.pipeline.data_preparation import DataPreparationPipeline\n",
    "from src.data.query.questdb_market_data_query import QuestDBMarketDataQuery\n",
    "\n",
    "print(\"📚 Libraries imported successfully!\")\n",
    "print(f\"📁 Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03325892",
   "metadata": {},
   "source": [
    "## 1. Explore Available Data\n",
    "\n",
    "Let's first explore what data is available in our QuestDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50fb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize query service\n",
    "query_service = QuestDBMarketDataQuery()\n",
    "\n",
    "# Get available symbols\n",
    "print(\"🔍 Fetching available symbols...\")\n",
    "try:\n",
    "    available_symbols = query_service.get_available_symbols()\n",
    "    print(f\"✅ Found {len(available_symbols)} symbols\")\n",
    "    \n",
    "    # Display first 10 symbols\n",
    "    print(\"\\n📋 Available symbols (first 10):\")\n",
    "    for i, symbol in enumerate(available_symbols[:10]):\n",
    "        print(f\"   {i+1}. {symbol}\")\n",
    "    \n",
    "    if len(available_symbols) > 10:\n",
    "        print(f\"   ... and {len(available_symbols) - 10} more\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error fetching symbols: {e}\")\n",
    "    available_symbols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b952301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data range for a specific symbol\n",
    "if available_symbols:\n",
    "    sample_symbol = available_symbols[0]  # Use first available symbol\n",
    "    print(f\"📊 Checking data range for {sample_symbol}...\")\n",
    "    \n",
    "    try:\n",
    "        data_range = query_service.get_data_range(sample_symbol)\n",
    "        print(f\"✅ Data range information:\")\n",
    "        print(f\"   Symbol: {data_range['symbol']}\")\n",
    "        print(f\"   Exchange: {data_range['exchange']}\")\n",
    "        print(f\"   Start: {data_range['start_time']}\")\n",
    "        print(f\"   End: {data_range['end_time']}\")\n",
    "        print(f\"   Records: {data_range['total_records']:,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error getting data range: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2a013",
   "metadata": {},
   "source": [
    "## 2. Configure Data Parameters\n",
    "\n",
    "Now let's configure the data parameters for our strategy development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a55e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA CONFIGURATION PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# Basic Data Selection\n",
    "SYMBOL = \"BTC-USDT-SWAP\"  # Change this to your desired symbol\n",
    "EXCHANGE = \"OKX\"\n",
    "TIMEFRAME = \"1h\"  # Options: \"1m\", \"5m\", \"15m\", \"30m\", \"1h\", \"4h\", \"1d\"\n",
    "\n",
    "# Date Range (adjust these dates based on your needs)\n",
    "START_DATE = datetime(2023, 1, 1, tzinfo=timezone.utc)\n",
    "END_DATE = datetime(2024, 6, 30, tzinfo=timezone.utc)\n",
    "\n",
    "# Data Splitting Configuration\n",
    "TRAIN_PCT = 0.6        # 60% for training\n",
    "VALIDATION_PCT = 0.2   # 20% for validation (set to None if not needed)\n",
    "TEST_PCT = 0.2         # 20% for testing (set to None if not needed)\n",
    "PURGE_DAYS = 1         # Days to purge between splits\n",
    "\n",
    "# Data Quality Parameters\n",
    "MIN_DATA_POINTS = 1000\n",
    "MAX_GAP_MINUTES = 120  # Maximum acceptable gap in minutes\n",
    "OUTLIER_THRESHOLD = 5.0  # Standard deviations for outlier detection\n",
    "\n",
    "# Configuration Name and Description\n",
    "CONFIG_NAME = \"btc_usdt_swap_1h_2023_2024\"\n",
    "DESCRIPTION = \"BTC-USDT-SWAP 1h data for 2023-2024 strategy development\"\n",
    "\n",
    "print(\"📝 Configuration parameters set!\")\n",
    "print(f\"   Symbol: {SYMBOL}\")\n",
    "print(f\"   Exchange: {EXCHANGE}\")\n",
    "print(f\"   Timeframe: {TIMEFRAME}\")\n",
    "print(f\"   Period: {START_DATE.strftime('%Y-%m-%d')} to {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "print(f\"   Splits: Train {TRAIN_PCT:.0%} | Val {VALIDATION_PCT:.0%} | Test {TEST_PCT:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d78c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data configuration\n",
    "split_config = DataSplitConfig(\n",
    "    train_pct=TRAIN_PCT,\n",
    "    validation_pct=VALIDATION_PCT,\n",
    "    test_pct=TEST_PCT,\n",
    "    purge_days=PURGE_DAYS\n",
    ")\n",
    "\n",
    "data_config = DataConfig(\n",
    "    symbol=SYMBOL,\n",
    "    exchange=EXCHANGE,\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    timeframe=TIMEFRAME,\n",
    "    split_config=split_config,\n",
    "    min_data_points=MIN_DATA_POINTS,\n",
    "    max_gap_minutes=MAX_GAP_MINUTES,\n",
    "    outlier_std_threshold=OUTLIER_THRESHOLD,\n",
    "    config_name=CONFIG_NAME,\n",
    "    description=DESCRIPTION\n",
    ")\n",
    "\n",
    "print(\"✅ Data configuration created!\")\n",
    "print(\"\\n\" + str(data_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show split dates\n",
    "split_dates = data_config.get_split_dates()\n",
    "\n",
    "print(\"📅 Data Split Schedule:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for split_name, (start, end) in split_dates.items():\n",
    "    duration = end - start\n",
    "    print(f\"{split_name.upper():12} {start.strftime('%Y-%m-%d %H:%M')} to {end.strftime('%Y-%m-%d %H:%M')} ({duration.days} days)\")\n",
    "\n",
    "print(f\"\\n📊 Expected data points: {data_config.get_expected_data_points():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d67cc",
   "metadata": {},
   "source": [
    "## 3. Preview Data Sample\n",
    "\n",
    "Let's fetch a small sample of data to verify everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a small sample for preview\n",
    "sample_end = START_DATE + timedelta(days=7)  # 1 week sample\n",
    "\n",
    "print(f\"📥 Fetching sample data from {START_DATE.strftime('%Y-%m-%d')} to {sample_end.strftime('%Y-%m-%d')}...\")\n",
    "\n",
    "try:\n",
    "    sample_data = query_service.get_market_data(\n",
    "        symbol=SYMBOL,\n",
    "        start_date=START_DATE,\n",
    "        end_date=sample_end,\n",
    "        timeframe=TIMEFRAME,\n",
    "        exchange=EXCHANGE\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Sample data retrieved: {len(sample_data)} records\")\n",
    "    \n",
    "    if len(sample_data) > 0:\n",
    "        print(\"\\n📊 Sample data preview:\")\n",
    "        print(sample_data.head(10))\n",
    "        \n",
    "        print(\"\\n📈 Basic statistics:\")\n",
    "        print(sample_data.select([\n",
    "            pl.col(\"open\").mean().alias(\"avg_open\"),\n",
    "            pl.col(\"high\").max().alias(\"max_high\"),\n",
    "            pl.col(\"low\").min().alias(\"min_low\"),\n",
    "            pl.col(\"close\").mean().alias(\"avg_close\"),\n",
    "            pl.col(\"volume\").mean().alias(\"avg_volume\")\n",
    "        ]))\n",
    "    else:\n",
    "        print(\"❌ No sample data found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error fetching sample data: {e}\")\n",
    "    sample_data = pl.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e823953",
   "metadata": {},
   "source": [
    "## 4. Data Quality Preview\n",
    "\n",
    "Let's check the quality of our sample data before proceeding with the full preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d04ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sample_data) > 0:\n",
    "    # Create a pipeline for quality checking\n",
    "    sample_config = DataConfig(\n",
    "        symbol=SYMBOL,\n",
    "        exchange=EXCHANGE,\n",
    "        start_date=START_DATE,\n",
    "        end_date=sample_end,\n",
    "        timeframe=TIMEFRAME,\n",
    "        split_config=split_config,\n",
    "        min_data_points=50,  # Lower threshold for sample\n",
    "        max_gap_minutes=MAX_GAP_MINUTES,\n",
    "        outlier_std_threshold=OUTLIER_THRESHOLD,\n",
    "        config_name=\"sample_check\"\n",
    "    )\n",
    "    \n",
    "    sample_pipeline = DataPreparationPipeline(sample_config)\n",
    "    \n",
    "    print(\"🔍 Checking sample data quality...\")\n",
    "    sample_pipeline.print_validation_report(sample_data)\n",
    "else:\n",
    "    print(\"⚠️  No sample data available for quality check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9095a",
   "metadata": {},
   "source": [
    "## 5. Full Data Preparation\n",
    "\n",
    "Now let's prepare the complete dataset for strategy development.\n",
    "\n",
    "⚠️ **Warning**: This may take several minutes depending on the data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm before proceeding\n",
    "print(\"🚨 READY TO PREPARE FULL DATASET\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Symbol: {SYMBOL}\")\n",
    "print(f\"Period: {START_DATE.strftime('%Y-%m-%d')} to {END_DATE.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Expected records: ~{data_config.get_expected_data_points():,}\")\n",
    "print(f\"Configuration: {CONFIG_NAME}\")\n",
    "print(\"\\n💡 This will:\")\n",
    "print(\"   1. Download full dataset from QuestDB\")\n",
    "print(\"   2. Validate data quality\")\n",
    "print(\"   3. Clean and process data\")\n",
    "print(\"   4. Split into train/validation/test sets\")\n",
    "print(\"   5. Save datasets to disk\")\n",
    "print(\"\\n⏱️  This may take several minutes...\")\n",
    "\n",
    "# Set to True to proceed\n",
    "PROCEED_WITH_PREPARATION = True  # Change to True when ready\n",
    "\n",
    "if PROCEED_WITH_PREPARATION:\n",
    "    print(\"\\n🚀 Starting full data preparation...\")\n",
    "else:\n",
    "    print(\"\\n⏸️  Preparation paused. Set PROCEED_WITH_PREPARATION = True to continue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740bfd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCEED_WITH_PREPARATION:\n",
    "    # Create preparation pipeline\n",
    "    pipeline = DataPreparationPipeline(data_config)\n",
    "    \n",
    "    try:\n",
    "        # Execute full preparation\n",
    "        prepared_datasets = pipeline.prepare_data(save_to_disk=True)\n",
    "        \n",
    "        print(\"\\n🎉 Data preparation completed successfully!\")\n",
    "        print(f\"\\n📁 Datasets saved to: data/processed/{CONFIG_NAME}/\")\n",
    "        \n",
    "        # Show final summary\n",
    "        for split_name, dataset in prepared_datasets.items():\n",
    "            if len(dataset) > 0:\n",
    "                print(f\"\\n📊 {split_name.upper()} SET:\")\n",
    "                print(f\"   Records: {len(dataset):,}\")\n",
    "                print(f\"   Period: {dataset['timestamp'].min()} to {dataset['timestamp'].max()}\")\n",
    "                print(f\"   Columns: {', '.join(dataset.columns)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error during preparation: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565d228",
   "metadata": {},
   "source": [
    "## 6. Verification and Next Steps\n",
    "\n",
    "Let's verify the prepared datasets and discuss next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f972df76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all prepared datasets\n",
    "print(\"📋 All prepared datasets:\")\n",
    "datasets = DataPreparationPipeline.list_prepared_datasets()\n",
    "\n",
    "for i, dataset_name in enumerate(datasets, 1):\n",
    "    print(f\"   {i}. {dataset_name}\")\n",
    "\n",
    "if not datasets:\n",
    "    print(\"   No datasets found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ddf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify the prepared dataset\n",
    "if CONFIG_NAME in datasets:\n",
    "    print(f\"\\n🔍 Verifying prepared dataset: {CONFIG_NAME}\")\n",
    "    \n",
    "    # Get dataset info\n",
    "    dataset_info = pipeline.get_dataset_info(CONFIG_NAME)\n",
    "    \n",
    "    print(f\"\\n📊 Dataset Information:\")\n",
    "    print(f\"   Symbol: {dataset_info['symbol']}\")\n",
    "    print(f\"   Exchange: {dataset_info['exchange']}\")\n",
    "    print(f\"   Timeframe: {dataset_info['timeframe']}\")\n",
    "    print(f\"   Prepared: {dataset_info['prepared_at']}\")\n",
    "    \n",
    "    print(f\"\\n📈 Dataset Splits:\")\n",
    "    for split_name, split_info in dataset_info['datasets'].items():\n",
    "        print(f\"   {split_name.upper()}:\")\n",
    "        print(f\"     Records: {split_info['records']:,}\")\n",
    "        print(f\"     Period: {split_info['start_date']} to {split_info['end_date']}\")\n",
    "        print(f\"     File: {split_info['filename']}\")\n",
    "    \n",
    "    # Test loading a split\n",
    "    train_data = pipeline.load_prepared_dataset(CONFIG_NAME, \"train\")\n",
    "    print(f\"\\n✅ Successfully loaded training set: {len(train_data):,} records\")\n",
    "    print(f\"   Columns: {', '.join(train_data.columns)}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n❌ Dataset {CONFIG_NAME} not found in prepared datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf897d",
   "metadata": {},
   "source": [
    "## 7. Next Steps in Development Workflow\n",
    "\n",
    "Now that your data is prepared, here are the next steps in the development workflow:\n",
    "\n",
    "### Phase 1: Complete ✅\n",
    "- ✅ Data Collection & Preparation\n",
    "- ✅ Data Quality Validation\n",
    "- ✅ Data Splitting Strategy\n",
    "\n",
    "### Phase 2: Strategy Development & Optimization\n",
    "Navigate to the following notebooks:\n",
    "1. **Feature Engineering**: `notebooks/01_data_exploration/feature_exploration.ipynb`\n",
    "2. **Strategy Development**: `notebooks/02_strategy_development/strategy_prototyping.ipynb`\n",
    "3. **Parameter Optimization**: `notebooks/03_optimization/parameter_optimization.ipynb`\n",
    "\n",
    "### Using Your Prepared Data\n",
    "In subsequent notebooks, you can load your prepared data like this:\n",
    "\n",
    "```python\n",
    "from src.data.pipeline.data_preparation import DataPreparationPipeline\n",
    "\n",
    "pipeline = DataPreparationPipeline(data_config)\n",
    "train_data = pipeline.load_prepared_dataset(\"your_config_name\", \"train\")\n",
    "validation_data = pipeline.load_prepared_dataset(\"your_config_name\", \"validation\")\n",
    "test_data = pipeline.load_prepared_dataset(\"your_config_name\", \"test\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e41538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration for future reference\n",
    "if 'data_config' in locals():\n",
    "    config_path = data_config.save_config()\n",
    "    print(f\"💾 Configuration saved to: {config_path}\")\n",
    "    \n",
    "    print(\"\\n📋 Configuration Summary:\")\n",
    "    print(json.dumps(data_config.to_dict(), indent=2, default=str))\n",
    "\n",
    "print(\"\\n🎯 Ready to proceed to Phase 2: Strategy Development!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
